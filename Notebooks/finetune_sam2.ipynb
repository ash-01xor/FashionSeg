{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets import load_dataset\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_REPO_NAME = \"peaceAsh/fashion_sam_dataset_v2\"\n",
    "COCO_DATASET = \"peaceAsh/fashion_seg_coco_dataset\"\n",
    "JSON_FILE = \"result.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionSAMDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, coco_api, image_size=1024, max_points=32):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.coco_api = coco_api\n",
    "        self.image_size = image_size\n",
    "        self.max_points = max_points\n",
    "\n",
    "        coco_images = self.coco_api.loadImgs(self.coco_api.getImgIds())\n",
    "        self.filename_to_id = {img['file_name']: img['id'] for img in coco_images}\n",
    "        self.filenames = sorted(self.filename_to_id.keys())\n",
    "        self.idx_to_filename = {i: fname for i, fname in enumerate(self.filenames)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def _sample_points_from_ann(self, ann, scale_x, scale_y, original_w, original_h):\n",
    "        individual_mask = self.coco_api.annToMask(ann)\n",
    "        # Erode to avoid edge points\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        eroded_mask = cv2.erode(individual_mask, kernel, iterations=1)\n",
    "        coords = np.argwhere(eroded_mask > 0)  # rows, cols\n",
    "\n",
    "        if len(coords) == 0:\n",
    "            return []\n",
    "\n",
    "        # pick between 1 and 3 points\n",
    "        num_points = np.random.randint(1, 4)\n",
    "        num_to_pick = min(num_points, len(coords))\n",
    "        selected_coords = coords[np.random.choice(len(coords), size=num_to_pick, replace=False)]\n",
    "\n",
    "        pts = []\n",
    "        for yx in selected_coords:\n",
    "            row, col = int(yx[0]), int(yx[1])   # row=y, col=x\n",
    "            x_resized = int(col * scale_x)\n",
    "            y_resized = int(row * scale_y)\n",
    "            # clamp to resized image bounds\n",
    "            x_resized = max(0, min(self.image_size - 1, x_resized))\n",
    "            y_resized = max(0, min(self.image_size - 1, y_resized))\n",
    "            pts.append([x_resized, y_resized])\n",
    "        return pts\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.hf_dataset[idx]\n",
    "        image_pil = entry['image'].convert(\"RGB\")\n",
    "        mask_pil = entry['mask'].convert(\"L\")\n",
    "\n",
    "        image_np = np.array(image_pil)          # H, W, 3\n",
    "        mask_np = np.array(mask_pil)            # H, W\n",
    "\n",
    "        original_h, original_w = image_np.shape[:2]\n",
    "\n",
    "        image_resized = cv2.resize(image_np, (self.image_size, self.image_size))\n",
    "        mask_resized = cv2.resize(mask_np, (self.image_size, self.image_size),\n",
    "                                  interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # compute scale factors actually used in resize\n",
    "        scale_x = self.image_size / float(original_w)\n",
    "        scale_y = self.image_size / float(original_h)\n",
    "\n",
    "        # Gather points from COCO annotations (1-3 points per instance)\n",
    "        points = []\n",
    "        filename = self.idx_to_filename.get(idx) \n",
    "        img_id = self.filename_to_id.get(filename)\n",
    "        if img_id is not None:\n",
    "            ann_ids = self.coco_api.getAnnIds(imgIds=img_id)\n",
    "            anns = self.coco_api.loadAnns(ann_ids)\n",
    "            for ann in anns:\n",
    "                pts = self._sample_points_from_ann(ann, scale_x, scale_y, original_w, original_h)\n",
    "                points.extend(pts)\n",
    "\n",
    "        if len(points) > self.max_points:\n",
    "            points = points[:self.max_points]\n",
    "\n",
    "        # create padded points array of shape (max_points, 2), pad with -1\n",
    "        pts_arr = np.full((self.max_points, 2), -1, dtype=np.float32)\n",
    "        point_labels = np.zeros((self.max_points,), dtype=np.int64)  \n",
    "        for i, p in enumerate(points):\n",
    "            pts_arr[i, 0] = p[0]\n",
    "            pts_arr[i, 1] = p[1]\n",
    "            point_labels[i] = 1\n",
    "\n",
    "        image_tensor = torch.tensor(image_resized).permute(2, 0, 1).to(torch.uint8)  # (3, H, W)\n",
    "        mask_tensor = torch.tensor(mask_resized).unsqueeze(0).to(torch.uint8)       # (1, H, W)\n",
    "        points_tensor = torch.tensor(pts_arr, dtype=torch.float32)                 # (max_points, 2)\n",
    "        point_labels_tensor = torch.tensor(point_labels, dtype=torch.int64)       # (max_points,)\n",
    "\n",
    "        return {\n",
    "            \"image\": image_tensor,\n",
    "            \"mask\": mask_tensor,\n",
    "            \"points\": points_tensor,\n",
    "            \"point_labels\": point_labels_tensor\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a61bb18eab6488580106fffd9205062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "result.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "downloaded_coco_path = hf_hub_download(\n",
    "    repo_id=COCO_DATASET,\n",
    "    filename=JSON_FILE,\n",
    "    repo_type=\"dataset\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "fashion_ds = load_dataset(HF_REPO_NAME, split='train')\n",
    "coco = COCO(downloaded_coco_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = fashion_ds.train_test_split(test_size=0.1)\n",
    "train_hf_dataset = train_val_split['train']\n",
    "val_hf_dataset = train_val_split['test']\n",
    "\n",
    "train_dataset = FashionSAMDataset(train_hf_dataset, coco)\n",
    "val_dataset = FashionSAMDataset(val_hf_dataset, coco)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
