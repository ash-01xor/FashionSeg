{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import math \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets import load_dataset \n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(),\"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess_data import load_and_split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_REPO_NAME = \"peaceAsh/fashion_sam_dataset_v2\"\n",
    "COCO_DATASET = \"peaceAsh/fashion_seg_coco_dataset\"\n",
    "JSON_FILE = \"result.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_ds = load_and_split_dataset(HF_REPO_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'mask', 'filename'],\n",
       "        num_rows: 13\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'mask', 'filename'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'mask', 'filename'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_path = hf_hub_download(\n",
    "    repo_id=COCO_DATASET,\n",
    "    filename=JSON_FILE,\n",
    "    repo_type=\"dataset\" \n",
    ")\n",
    "\n",
    "coco = COCO(coco_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_imgs = coco.loadImgs(coco.getImgIds())\n",
    "filename_to_ids = {img['file_name'] : img['id'] for img in coco_imgs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ce8d1b46254ffd85f703598463cad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea13691d85941c0b75d8e9070fea08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:13\n",
      "Total training instances:37\n",
      "Number of images:1\n",
      "Total validation instances:4\n"
     ]
    }
   ],
   "source": [
    "def create_instance_list(dataset,coco,filename_to_ids):\n",
    "    instance_list = []\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        item = dataset[i]\n",
    "        base_filename = item['filename']\n",
    "        coco_filename = base_filename.split('/')[-1]\n",
    "        img_id = filename_to_ids[coco_filename]\n",
    "        if img_id is not None:\n",
    "            anns_ids = coco.getAnnIds(imgIds=img_id)\n",
    "            anns = coco.loadAnns(anns_ids)\n",
    "            for ann in anns:\n",
    "                instance_list.append({\n",
    "                    \"dataset_idx\":i,\n",
    "                    \"annotation\":ann\n",
    "                })\n",
    "    return instance_list\n",
    "\n",
    "\n",
    "\n",
    "train_instances = create_instance_list(fashion_ds['train'],coco,filename_to_ids)\n",
    "val_instances = create_instance_list(fashion_ds['validation'],coco,filename_to_ids)\n",
    "\n",
    "print(f\"Number of images:{len(fashion_ds['train'])}\")\n",
    "print(f\"Total training instances:{len(train_instances)}\")\n",
    "print(f\"Number of images:{len(fashion_ds['validation'])}\")\n",
    "print(f\"Total validation instances:{len(val_instances)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionSAMDataset(Dataset):\n",
    "    def __init__(self,dataset,instance_list,coco_api,image_size=1024,num_pts_per_instance=3):\n",
    "        self.dataset = dataset\n",
    "        self.instance_list = instance_list\n",
    "        self.coco_api = coco_api\n",
    "        self.image_size = image_size\n",
    "        self.num_pts_per_instance = num_pts_per_instance\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instance_list)\n",
    "    \n",
    "    def _resize_and_pad(self,image,is_mask=False):\n",
    "        target_size = self.image_size\n",
    "        if is_mask:\n",
    "            h,w = image.shape\n",
    "            scale = target_size / max(h,w)\n",
    "            new_h , new_w = int(h*scale), int(w*scale)\n",
    "            resized_image = cv2.resize(image.astype(np.uint8), (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "            padded_image = np.zeros((target_size, target_size), dtype=np.float32)\n",
    "        else:\n",
    "            image.thumbnail((target_size, target_size), Image.Resampling.LANCZOS)\n",
    "            new_w, new_h = image.size\n",
    "            scale = target_size / max(image.size)\n",
    "            padded_image = Image.new(\"RGB\", (target_size, target_size), (0, 0, 0))\n",
    "            \n",
    "        top = (target_size - new_h) // 2\n",
    "        left = (target_size - new_w) // 2\n",
    "\n",
    "        if is_mask:\n",
    "            padded_image[top:top + new_h, left:left + new_w] = resized_image\n",
    "        else:\n",
    "            padded_image.paste(image, (left, top))\n",
    "            padded_image = np.array(padded_image, dtype=np.uint8)\n",
    "\n",
    "        return padded_image, (scale, left, top)\n",
    "    \n",
    "    def _sample_points(self, mask):\n",
    "        if np.max(mask) == 0:\n",
    "            return np.empty((0, 2), dtype=np.float32)\n",
    "\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        eroded_mask = cv2.erode(mask, kernel, iterations=1)\n",
    "        \n",
    "        foreground_coords = np.argwhere(eroded_mask > 0)\n",
    "        if len(foreground_coords) == 0: \n",
    "            foreground_coords = np.argwhere(mask > 0)\n",
    "        \n",
    "        if len(foreground_coords) == 0:\n",
    "            return np.empty((0, 2), dtype=np.float32)\n",
    "\n",
    "        num_available = len(foreground_coords)\n",
    "        points_to_sample = min(self.num_pts_per_instance, num_available)\n",
    "        \n",
    "        sampled_indices = np.random.choice(num_available, points_to_sample, replace=False)\n",
    "        coords = foreground_coords[sampled_indices][:, ::-1].astype(np.float32)\n",
    "        return coords\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        instance_info = self.instance_list[idx]\n",
    "        dataset_idx = instance_info['dataset_idx']\n",
    "        annotation = instance_info['annotation']\n",
    "        image_pil = self.dataset[dataset_idx]['image'].convert(\"RGB\")\n",
    "        instance_mask = self.coco_api.annToMask(annotation)\n",
    "\n",
    "        image_padded, (scale, pad_left, pad_top) = self._resize_and_pad(image_pil, is_mask=False)\n",
    "        mask_padded, _ = self._resize_and_pad(instance_mask, is_mask=True)\n",
    "        points = self._sample_points(instance_mask)\n",
    "        if points.shape[0] > 0:\n",
    "            points = points * scale\n",
    "            points[:, 0] += pad_left\n",
    "            points[:, 1] += pad_top\n",
    "\n",
    "        image_tensor = torch.tensor(image_padded).permute(2, 0, 1).float()\n",
    "        mask_tensor = torch.tensor(mask_padded).unsqueeze(0).float()\n",
    "        points_tensor = torch.tensor(points).unsqueeze(1) \n",
    "        labels_tensor = torch.ones(points_tensor.shape[0], dtype=torch.int64)\n",
    "\n",
    "        return {\n",
    "            \"image\": image_tensor,\n",
    "            \"mask\": mask_tensor,\n",
    "            \"points\": points_tensor,\n",
    "            \"point_labels\": labels_tensor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sam_collate_fn(batch):\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    masks = torch.stack([item['mask'] for item in batch])\n",
    "    points = [item['points'] for item in batch]\n",
    "    point_labels = [item['point_labels'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'image': images,\n",
    "        'mask': masks,\n",
    "        'points': points,\n",
    "        'point_labels': point_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FashionSAMDataset(\n",
    "    dataset=fashion_ds['train'],\n",
    "    instance_list=train_instances,\n",
    "    coco_api=coco\n",
    ")\n",
    "\n",
    "val_dataset = FashionSAMDataset(   \n",
    "    dataset=fashion_ds['validation'],\n",
    "    instance_list=val_instances,\n",
    "    coco_api=coco\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=sam_collate_fn,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=sam_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape:torch.Size([2, 3, 1024, 1024])\n",
      "Mask batch shape:torch.Size([2, 1, 1024, 1024])\n",
      "Points batch length:2\n",
      "Point labels batch length:2\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(iter(train_dataloader))\n",
    "print(f\"Image batch shape:{first_batch['image'].shape}\")\n",
    "print(f\"Mask batch shape:{first_batch['mask'].shape}\")\n",
    "print(f\"Points batch length:{len(first_batch['points'])}\")\n",
    "print(f\"Point labels batch length:{len(first_batch['point_labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
