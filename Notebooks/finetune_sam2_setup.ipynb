{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import math \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets import load_dataset \n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(),\"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess_data import load_and_split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_REPO_NAME = \"peaceAsh/fashion_sam_dataset_v2\"\n",
    "COCO_DATASET = \"peaceAsh/fashion_seg_coco_dataset\"\n",
    "JSON_FILE = \"result.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_ds = load_and_split_dataset(HF_REPO_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'mask', 'filename'],\n",
       "        num_rows: 13\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'mask', 'filename'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'mask', 'filename'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_path = hf_hub_download(\n",
    "    repo_id=COCO_DATASET,\n",
    "    filename=JSON_FILE,\n",
    "    repo_type=\"dataset\" \n",
    ")\n",
    "\n",
    "coco = COCO(coco_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_imgs = coco.loadImgs(coco.getImgIds())\n",
    "filename_to_ids = {img['file_name'] : img['id'] for img in coco_imgs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ce8d1b46254ffd85f703598463cad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea13691d85941c0b75d8e9070fea08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:13\n",
      "Total training instances:37\n",
      "Number of images:1\n",
      "Total validation instances:4\n"
     ]
    }
   ],
   "source": [
    "def create_instance_list(dataset,coco,filename_to_ids):\n",
    "    instance_list = []\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        item = dataset[i]\n",
    "        base_filename = item['filename']\n",
    "        coco_filename = base_filename.split('/')[-1]\n",
    "        img_id = filename_to_ids[coco_filename]\n",
    "        if img_id is not None:\n",
    "            anns_ids = coco.getAnnIds(imgIds=img_id)\n",
    "            anns = coco.loadAnns(anns_ids)\n",
    "            for ann in anns:\n",
    "                instance_list.append({\n",
    "                    \"dataset_idx\":i,\n",
    "                    \"annotation\":ann\n",
    "                })\n",
    "    return instance_list\n",
    "\n",
    "\n",
    "\n",
    "train_instances = create_instance_list(fashion_ds['train'],coco,filename_to_ids)\n",
    "val_instances = create_instance_list(fashion_ds['validation'],coco,filename_to_ids)\n",
    "\n",
    "print(f\"Number of images:{len(fashion_ds['train'])}\")\n",
    "print(f\"Total training instances:{len(train_instances)}\")\n",
    "print(f\"Number of images:{len(fashion_ds['validation'])}\")\n",
    "print(f\"Total validation instances:{len(val_instances)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionSAMDataset(Dataset):\n",
    "    def __init__(self,dataset,instance_list,coco_api,image_size=1024,num_pts_per_instance=3):\n",
    "        self.dataset = dataset\n",
    "        self.instance_list = instance_list\n",
    "        self.coco_api = coco_api\n",
    "        self.image_size = image_size\n",
    "        self.num_pts_per_instance = num_pts_per_instance\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instance_list)\n",
    "    \n",
    "    def _resize_and_pad(self,image,is_mask=False):\n",
    "        target_size = self.image_size\n",
    "        if is_mask:\n",
    "            h,w = image.shape\n",
    "            scale = target_size / max(h,w)\n",
    "            new_h , new_w = int(h*scale), int(w*scale)\n",
    "            resized_image = cv2.resize(image.astype(np.uint8), (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "            padded_image = np.zeros((target_size, target_size), dtype=np.float32)\n",
    "        else:\n",
    "            image.thumbnail((target_size, target_size), Image.Resampling.LANCZOS)\n",
    "            new_w, new_h = image.size\n",
    "            scale = target_size / max(image.size)\n",
    "            padded_image = Image.new(\"RGB\", (target_size, target_size), (0, 0, 0))\n",
    "            \n",
    "        top = (target_size - new_h) // 2\n",
    "        left = (target_size - new_w) // 2\n",
    "\n",
    "        if is_mask:\n",
    "            padded_image[top:top + new_h, left:left + new_w] = resized_image\n",
    "        else:\n",
    "            padded_image.paste(image, (left, top))\n",
    "            padded_image = np.array(padded_image, dtype=np.uint8)\n",
    "\n",
    "        return padded_image, (scale, left, top)\n",
    "    \n",
    "    def _sample_points(self, mask):\n",
    "        if np.max(mask) == 0:\n",
    "            return np.empty((0, 2), dtype=np.float32)\n",
    "\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        eroded_mask = cv2.erode(mask, kernel, iterations=1)\n",
    "        \n",
    "        foreground_coords = np.argwhere(eroded_mask > 0)\n",
    "        if len(foreground_coords) == 0: \n",
    "            foreground_coords = np.argwhere(mask > 0)\n",
    "        \n",
    "        if len(foreground_coords) == 0:\n",
    "            return np.empty((0, 2), dtype=np.float32)\n",
    "\n",
    "        num_available = len(foreground_coords)\n",
    "        points_to_sample = min(self.num_pts_per_instance, num_available)\n",
    "        \n",
    "        sampled_indices = np.random.choice(num_available, points_to_sample, replace=False)\n",
    "        coords = foreground_coords[sampled_indices][:, ::-1].astype(np.float32)\n",
    "        return coords\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        instance_info = self.instance_list[idx]\n",
    "        dataset_idx = instance_info['dataset_idx']\n",
    "        annotation = instance_info['annotation']\n",
    "        image_pil = self.dataset[dataset_idx]['image'].convert(\"RGB\")\n",
    "        instance_mask = self.coco_api.annToMask(annotation)\n",
    "\n",
    "        image_padded, (scale, pad_left, pad_top) = self._resize_and_pad(image_pil, is_mask=False)\n",
    "        mask_padded, _ = self._resize_and_pad(instance_mask, is_mask=True)\n",
    "        points = self._sample_points(instance_mask)\n",
    "        if points.shape[0] > 0:\n",
    "            points = points * scale\n",
    "            points[:, 0] += pad_left\n",
    "            points[:, 1] += pad_top\n",
    "\n",
    "        image_tensor = torch.tensor(image_padded).permute(2, 0, 1).float()\n",
    "        mask_tensor = torch.tensor(mask_padded).unsqueeze(0).float()\n",
    "        points_tensor = torch.tensor(points).unsqueeze(1) \n",
    "        labels_tensor = torch.ones(points_tensor.shape[0], dtype=torch.int64)\n",
    "\n",
    "        return {\n",
    "            \"image\": image_tensor,\n",
    "            \"mask\": mask_tensor,\n",
    "            \"points\": points_tensor,\n",
    "            \"point_labels\": labels_tensor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sam_collate_fn(batch):\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    masks = torch.stack([item['mask'] for item in batch])\n",
    "    points = [item['points'] for item in batch]\n",
    "    point_labels = [item['point_labels'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'image': images,\n",
    "        'mask': masks,\n",
    "        'points': points,\n",
    "        'point_labels': point_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FashionSAMDataset(\n",
    "    dataset=fashion_ds['train'],\n",
    "    instance_list=train_instances,\n",
    "    coco_api=coco\n",
    ")\n",
    "\n",
    "val_dataset = FashionSAMDataset(   \n",
    "    dataset=fashion_ds['validation'],\n",
    "    instance_list=val_instances,\n",
    "    coco_api=coco\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=sam_collate_fn,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=sam_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape:torch.Size([2, 3, 1024, 1024])\n",
      "Mask batch shape:torch.Size([2, 1, 1024, 1024])\n",
      "Points batch length:2\n",
      "Point labels batch length:2\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(iter(train_dataloader))\n",
    "print(f\"Image batch shape:{first_batch['image'].shape}\")\n",
    "print(f\"Mask batch shape:{first_batch['mask'].shape}\")\n",
    "print(f\"Points batch length:{len(first_batch['points'])}\")\n",
    "print(f\"Point labels batch length:{len(first_batch['point_labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_batch(batch):\n",
    "\n",
    "    images = batch['image']  # (B, 3, H, W)\n",
    "    masks = batch['mask']    # (B, 1, H, W)\n",
    "    points = batch['points'] # list of (N, 1, 2)\n",
    "    point_labels = batch['point_labels'] # list of (N,)\n",
    "\n",
    "    B = images.shape[0]\n",
    "\n",
    "    for i in range(B):\n",
    "        img = images[i].permute(1, 2, 0).cpu().numpy().astype('uint8')\n",
    "        mask = masks[i][0].cpu().numpy()\n",
    "        pts = points[i].squeeze(1).cpu().numpy()  # (N, 2)\n",
    "        \n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(mask, alpha=0.5)\n",
    "        if pts.shape[0] > 0:\n",
    "            plt.scatter(pts[:, 0], pts[:, 1], c='red', s=40, marker='o')\n",
    "        plt.title(f\"Sample {i}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "viz_batch(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model configs and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sam2_checkpoint = \"/home/FashionSeg/models/sam2.1_hiera_small.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_s.yaml\"\n",
    "\n",
    "sam2_model = build_sam2(model_cfg,sam2_checkpoint,device=device)\n",
    "predictor = SAM2ImagePredictor(sam2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 20\n",
    "CHECKPOINT_INTERVAL = 5\n",
    "EVAL_INTERVAL = 2\n",
    "\n",
    "EXP_NAME = \"fashion_sam2_finetune_v0\"\n",
    "CHECKPOINT_DIR = f\"../checkpoints/{EXP_NAME}\"\n",
    "LOGS_DIR =  f\"../logs/{EXP_NAME}\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(LOGS_DIR)\n",
    "print(f\"Starting experiment:{EXP_NAME}\\n Logs  saved in :{LOGS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.model.sam_mask_decoder.train(True)\n",
    "predictor.model.sam_prompt_encoder.train(True)\n",
    "predictor.model.image_encoder.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(inputs, targets):\n",
    "    inputs = inputs.sigmoid().flatten(1)\n",
    "    targets = targets.flatten(1)\n",
    "    numerator = 2 * (inputs * targets).sum(-1)\n",
    "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
    "    loss = 1 - (numerator + 1) / (denominator + 1)\n",
    "    return loss.mean()\n",
    "\n",
    "def sigmoid_focal_loss(inputs, targets, alpha: float = 0.25, gamma: float = 2):\n",
    "    prob = inputs.sigmoid()\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "    return loss.mean()\n",
    "\n",
    "def total_loss_fn(pred_masks, gt_masks, dice_weight=0.8, focal_weight=0.2):\n",
    "    \"\"\"Combined Dice and Focal loss.\"\"\"\n",
    "    dice = dice_loss(pred_masks, gt_masks)\n",
    "    focal = sigmoid_focal_loss(pred_masks, gt_masks)\n",
    "    total_loss = (dice_weight * dice) + (focal_weight * focal)\n",
    "    return total_loss, {\"dice\": dice, \"focal\": focal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(predictor.model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "scaler = torch.amp.GradScaler('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def forward(predictor,images,input_point,input_label):\n",
    "    batch_size = images.shape[0]\n",
    "    image_list = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Convert from (C, H, W) tensor to (H, W, C) numpy array\n",
    "        img = images[i].permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
    "        image_list.append(img)\n",
    "\n",
    "    input_points =  np.array(input_point, dtype=np.float32)\n",
    "    input_labels =  np.array(input_label, dtype=np.float32)\n",
    "    \n",
    "    predictor.set_image_batch(image_list)\n",
    "    mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(input_points, input_labels, box=None, mask_logits=None, normalize_coords=True)     \n",
    "    sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(points=(unnorm_coords, labels), boxes=None, masks=None)\n",
    "    high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "    low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "        image_embeddings=predictor._features[\"image_embed\"],\n",
    "        image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "        sparse_prompt_embeddings=sparse_embeddings,\n",
    "        dense_prompt_embeddings=dense_embeddings,\n",
    "        multimask_output=False,\n",
    "        repeat_image=False,\n",
    "        high_res_features=high_res_features\n",
    "    )\n",
    "    prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])  \n",
    "    prd_masks_logits = prd_masks[:, 0]\n",
    "    return prd_masks, prd_scores, torch.sigmoid(prd_masks_logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(predictor,train_dataloader,epoch,optimizer,scaler,scheduler,writer,accumulation_steps):\n",
    "    step = len(train_dataloader) * epoch\n",
    "    progress_bar = tqdm(train_dataloader, colour='green')\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        image = batch['image']\n",
    "        mask = batch['mask']\n",
    "        input_point = batch['points']\n",
    "        input_label = batch['point_labels']\n",
    "\n",
    "        gt_mask = mask.float().to(device)\n",
    "        step += 1\n",
    "        with torch.amp.autocast('cuda'): \n",
    "            prd_masks, prd_scores, prd_mask = forward(predictor, image, input_point, input_label)\n",
    "            loss , losses = total_loss_fn(prd_masks, gt_mask)\n",
    "            \n",
    "        loss = loss / accumulation_steps\n",
    "        predictor.model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item() * accumulation_steps:.3f}',\n",
    "                'Dice Loss': f'{losses[\"dice\"].item():.2f}',\n",
    "                'Focal Loss': f'{losses[\"focal\"].item():.2f}',\n",
    "            })\n",
    "\n",
    "        writer.add_scalar('train/total loss', loss.item() * accumulation_steps, step)\n",
    "        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], step)\n",
    "        for key, value in losses.items():\n",
    "            writer.add_scalar(f'train/{key}loss', value.item(), step)\n",
    "            \n",
    "    scheduler.step()  \n",
    "    \n",
    "def validate(predictor,val_dataloader,epoch,writer):\n",
    "    predictor.model.sam_mask_decoder.train(False)\n",
    "    total_val_loss = 0.0\n",
    "    for batch_idx, batch in enumerate(tqdm(val_dataloader, colour='blue')):\n",
    "            image = batch['image']\n",
    "            mask = batch['mask']\n",
    "            input_point = batch['points']\n",
    "            input_label = batch['point_labels']\n",
    "\n",
    "            gt_mask = mask.float().to(device)\n",
    "            prd_masks, prd_scores, prd_mask = forward(predictor, image, input_point, input_label)\n",
    "            val_loss, val_losses = total_loss_fn(prd_masks,gt_mask)\n",
    "            total_val_loss += val_loss.item()\n",
    "    total_val_loss /= len(val_dataloader)\n",
    "    predictor.model.sam_mask_decoder.train(True)\n",
    "    print(f\"Epoch {epoch + 1}, Val Loss: {total_val_loss:.3f}\")\n",
    "    writer.add_scalar('val/total_loss', total_val_loss, epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "        train_epochs(predictor, train_dataloader, epoch, optimizer, scaler, scheduler, writer, ACCUMULATION_STEPS)\n",
    "        if (epoch + 1) % EVAL_INTERVAL == 0:\n",
    "                validate(predictor, val_dataloader, epoch, writer)\n",
    "        if (epoch + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "                torch.save(predictor.model.state_dict(), f\"{CHECKPOINT_DIR}/model_epoch_{epoch + 1}.pt\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
